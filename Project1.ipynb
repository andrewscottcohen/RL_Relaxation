{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cognitive-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from confutils import get_initial_structure, get_dihedral_info, set_dihedrals_and_relax, relax_structure\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-potter",
   "metadata": {},
   "source": [
    "This is my first go at the group project. It currently can one 1 round and actually does minimize energy. That's progress. I'm getting an error with the modified tensor board which I didnt make and I don't know what to do with it. That's my next prject. If you want to have a go at fixing it, be my guest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-difference",
   "metadata": {},
   "source": [
    "# Things you can do:\n",
    "    \n",
    "   1) The convergence criterion doesn't work because it osicillates around a plateau and doesnt ever get to a point where it continually gets worse at the end. Please implement a new finishing criterion that takes the average over the last 100 steps. If the average over 100 steps stops changing significantly, we will define that plateau.\n",
    "   -This requires that you look at the percentage that it changes by at each step and select a percentage that defines a plateau.\n",
    "   \n",
    "   \n",
    "   2) You can pull your own version of this and try to define the intial functions and classes in .py files that can be imported. This got messy with the dependencies so I just shoved them all in here.\n",
    "   3) Find a way to evaluate the accuracy of the fit and prediction of the q values. Add this to the DQNagent. Ask it to plot error as it goes so we can see how it's performing. \n",
    "   \n",
    "   \n",
    "   4) plot all energy values as each game progresses. Do you see any local minima\n",
    "   \n",
    "   \n",
    "   5) the MAIN thing we need is to get it to run for multiple rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "expanded-agency",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_energy(angles):\n",
    "    \"\"\"Compute the energy of a cysteine molecule given dihedral angles\n",
    "    \n",
    "    Args:\n",
    "        angles: List of dihedral angles\n",
    "    Returns:\n",
    "        energy of the structure\n",
    "    \"\"\"\n",
    "    return set_dihedrals_and_relax(\n",
    "        cysteine,\n",
    "        zip(angles, dihedrals)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "royal-private",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Atoms(symbols='C2HCO2NSH6', pbc=False, constraint=FixInternals(_copy_init=[FixDihedral(111.01948280324353, [0, 1, 3, 5]), FixDihedral(48.18428332148011, [3, 1, 0, 7]), FixDihedral(-92.67739011337167, [0, 1, 6, 11]), FixDihedral(149.34099867428546, [1, 3, 5, 10]), FixDihedral(-27.606378634514414, [1, 0, 7, 13])], epsilon=1e-07), calculator=Calculator(...)),\n",
       " [DihedralInfo(chain=[0, 1, 3, 5], group={10, 3, 4, 5}),\n",
       "  DihedralInfo(chain=[3, 1, 0, 7], group={0, 7, 8, 9, 13}),\n",
       "  DihedralInfo(chain=[0, 1, 6, 11], group={11, 12, 6}),\n",
       "  DihedralInfo(chain=[1, 3, 5, 10], group={10, 5}),\n",
       "  DihedralInfo(chain=[1, 0, 7, 13], group={13, 7})],\n",
       " -19641.613722554852,\n",
       " array([111.0194828 ,  48.18428332, -92.67739011, 149.34099867,\n",
       "        -27.60637863]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WIN_REWARD=100\n",
    "MOVE_REWARD=1\n",
    "epsilon=.9\n",
    "eps_decay=.9998\n",
    "start_q_table= None\n",
    "learning=.1\n",
    "discount=.95\n",
    "def get_all(smiles,backbone_atoms,backbone_bonds):\n",
    "    atoms, bonds = get_initial_structure(smiles)\n",
    "   \n",
    "    start_energy = relax_structure(atoms)\n",
    "    diheds=[]\n",
    "    for i in backbone_bonds:\n",
    "        di=get_dihedral_info(bonds,i, backbone_atoms=backbone_atoms)\n",
    "        diheds.append(di)\n",
    "        \n",
    "    init_state=np.random.uniform(-180,180,len(diheds))\n",
    "    \n",
    "    energy=set_dihedrals_and_relax(\n",
    "        atoms,\n",
    "        zip(init_state, diheds)\n",
    "    )\n",
    "    \n",
    "    return atoms, diheds,energy,init_state\n",
    "\n",
    "      \n",
    "    \n",
    "get_all('C([C@@H](C(=O)O)N)S',[0, 1, 3, 5, 6, 7],[[1,3],[1, 0],[1, 6],[3, 5],[0, 7]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "accepted-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Molecule:\n",
    "    def __init__(self,smiles,backbone_atoms,backbone_bonds):\n",
    "        self.bbonds=backbone_bonds\n",
    "        self.n_choices=len(self.bbonds)\n",
    "        self.atoms, self.dihedrals, self.energy, self.angles=get_all(smiles,backbone_atoms,backbone_bonds)\n",
    "    def get_energy(self):\n",
    "        self.energy=set_dihedrals_and_relax(self.atoms,zip(self.angles, self.dihedrals))\n",
    "        return self.energy\n",
    "    choices=[]\n",
    "    def action(self,choice):\n",
    "        if choice %2 ==0:\n",
    "            self.angles[int(choice/2)]+=1.0\n",
    "            if abs(self.angles[int(choice/2)])>180:\n",
    "                self.angles[int(choice/2)]=self.angles[int(choice/2)]%180\n",
    "            \n",
    "        else:\n",
    "            self.angles[int(np.round(choice/2))]-=1.0\n",
    "            if abs(self.angles[int(choice/2)])>180:\n",
    "                self.angles[int(choice/2)]=self.angles[int(choice/2)]%180\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "class MolEnv:\n",
    "    mol = Molecule('C([C@@H](C(=O)O)N)S',[0, 1, 3, 5, 6, 7],[[1,3],[1, 0],[1, 6],[3, 5],[0, 7]])\n",
    "    SIZE = 360 #We dont have a finite size in this way\n",
    "    RETURN_IMAGES = False\n",
    "    ENERGY_REWARD=1\n",
    "    MIN_REWARD = 25\n",
    "    OBSERVATION_SPACE_VALUES = [360 for i in range(mol.n_choices)]\n",
    "    OBSERVATION_SPACE_VALUES.append(mol.n_choices)# 4 #WHAT IS GOING ON HERE\n",
    "    ACTION_SPACE_SIZE = mol.n_choices*2\n",
    "    OBSERVATION_SPACE_VALUES=tuple(OBSERVATION_SPACE_VALUES)\n",
    "    reward=0\n",
    "    def __init__(self):\n",
    "        self.energy_list=[]\n",
    "        self.strike=0\n",
    "        self.reward=0\n",
    "    def reset(self):\n",
    "        self.mol = Molecule('C([C@@H](C(=O)O)N)S',[0, 1, 3, 5, 6, 7],[[1,3],[1, 0],[1, 6],[3, 5],[0, 7]])\n",
    "        self.ener=self.mol.get_energy()\n",
    "        self.strikes=0\n",
    "        self.energy_list=[]\n",
    "        self.energy_list.append(self.ener)\n",
    "        OBSERVATION_SPACE_VALUES = [360 for i in range(self.mol.n_choices)]\n",
    "        OBSERVATION_SPACE_VALUES.append(self.mol.n_choices)# 4 #WHAT IS GOING ON HERE\n",
    "        \n",
    "        OBSERVATION_SPACE_VALUES=tuple(OBSERVATION_SPACE_VALUES)\n",
    "        self.episode_step = 0\n",
    "\n",
    "        if self.RETURN_IMAGES:\n",
    "            observation = np.array(self.get_image())\n",
    "        else:\n",
    "            observation = self.mol.angles\n",
    "        return observation\n",
    "\n",
    "    def step(self, action):\n",
    "        self.reward=0\n",
    "        self.episode_step += 1\n",
    "        \n",
    "        self.mol.action(action)\n",
    "        new_ener=self.mol.get_energy()\n",
    "        #print(new_ener,\"NEW EN\")\n",
    "        self.energy_list.append(new_ener)\n",
    "    \n",
    "\n",
    "        \n",
    "        new_observation = self.mol.angles\n",
    "\n",
    "        if abs(self.energy_list[self.episode_step])>abs(self.energy_list[self.episode_step-1]):\n",
    "            self.strike+=1\n",
    "        else:\n",
    "            self.strike=0\n",
    "            \n",
    "        if abs(self.energy_list[self.episode_step])<abs(self.energy_list[self.episode_step-1]):\n",
    "            self.reward=self.ENERGY_REWARD\n",
    "            \n",
    "        #plt.plot(np.arange(0,len(self.energy_list)),self.energy_list)\n",
    "\n",
    "        done = False\n",
    "        if self.strike>=2:\n",
    "            self.reward+=100\n",
    "            plt.plot(np.arange(0,len(self.energy_list)),self.energy_list)\n",
    "            done = True\n",
    "\n",
    "        return new_observation, self.reward, done\n",
    "\n",
    "    #def render(self):\n",
    "        #img = self.get_image()\n",
    "        #img = img.resize((300, 300))  # resizing so we can see our agent in all its glory.\n",
    "        #cv2.imshow(\"image\", np.array(img))  # show it!\n",
    "        #cv2.waitKey(1)\n",
    "\n",
    "    # FOR CNN #\n",
    "    #def get_image(self):\n",
    "        #env = np.zeros((self.SIZE, self.SIZE, 3), dtype=np.uint8)  # starts an rbg of our size\n",
    "        #env[self.food.x][self.food.y] = self.d[self.FOOD_N]  # sets the food location tile to green color\n",
    "        #env[self.enemy.x][self.enemy.y] = self.d[self.ENEMY_N]  # sets the enemy location to red\n",
    "        #env[self.player.x][self.player.y] = self.d[self.PLAYER_N]  # sets the player tile to blue\n",
    "        #img = Image.fromarray(env, 'RGB')  # reading to rgb. Apparently. Even tho color definitions are bgr. ???\n",
    "        #return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "numerous-bidder",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLAY_MEMORY_SIZE = 5000\n",
    "MIN_REPLAY_MEMORY_SIZE = 1000\n",
    "MODEL_NAME = \"256x2\"\n",
    "DISCOUNT=0.99\n",
    "MINIBATCH_SIZE = 64  # How many steps (samples) to use for training\n",
    "UPDATE_TARGET_EVERY = 5  # Terminal states (end of episodes)\n",
    "MIN_EPSILON=.5\n",
    "\n",
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self,env):\n",
    "\n",
    "        # main model  # gets trained every step\n",
    "        self.model = self.create_model()\n",
    "\n",
    "        # Target model this is what we .predict against every step\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "        self.target_update_counter = 0\n",
    "\n",
    "    def create_model(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        #model.add(Conv2D(256, (3, 3), input_shape=env.OBSERVATION_SPACE_VALUES))\n",
    "        #model.add(keras.Input(shape=(REPLAY_MEMORY_SIZE,a1.n_choices)))\n",
    "        model.add(keras.Input(shape=(a1.n_choices)))\n",
    "        model.add(Dense(a1.n_choices))\n",
    "\n",
    "        #model.add(Flatten())\n",
    "        \n",
    "        model.add(Dense(a1.n_choices,activation='linear'))\n",
    "\n",
    "        model.add(Dense(env.ACTION_SPACE_SIZE, activation='linear'))\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    def get_qs(self, state, step):\n",
    "        #print(*state.shape)\n",
    "        return self.model.predict(np.array(state).reshape(-1, *state.shape)/180)[0]\n",
    "    def train(self, terminal_state,step):\n",
    "        if len(self.replay_memory)<MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "    # Get a minibatch of random samples from memory replay table\n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "\n",
    "        # Get current states from minibatch, then query NN model for Q values\n",
    "        current_states = np.array([transition[0] for transition in minibatch])/180\n",
    "        current_qs_list = self.model.predict(current_states)\n",
    "\n",
    "        # Get future states from minibatch, then query NN model for Q values\n",
    "        # When using target network, query it, otherwise main network should be queried\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])/180\n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        # Now we need to enumerate our batches\n",
    "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "\n",
    "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
    "            # almost like with Q Learning, but we use just part of equation here\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            # Update Q value for given state\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "            # And append to our training data\n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "\n",
    "        # Fit on all samples as one batch, log only on terminal state\n",
    "        self.model.fit(np.array(X)/180, np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False)\n",
    "\n",
    "        # Update target network counter every episode\n",
    "                # Update target network counter every episode\n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        # If counter reaches set value, update target network with weights of main network\n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "meaning-smile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  24.18323376   54.72452303   81.79197813  138.00571811 -121.16517668]\n",
      "[  24.18323376   55.72452303   81.79197813  138.00571811 -121.16517668]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a1=Molecule('C([C@@H](C(=O)O)N)S',[0, 1, 3, 5, 6, 7],[[1,3],[1, 0],[1, 6],[3, 5],[0, 7]])\n",
    "print(a1.angles)\n",
    "a1.action(2)\n",
    "print(a1.angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-major",
   "metadata": {},
   "source": [
    "# This is the part that actually runs the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fancy-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=MolEnv()\n",
    "EPISODES = 5\n",
    "agent = DQNAgent(env)\n",
    "epsilon=.9\n",
    "eps_decay=.9998\n",
    "AGGREGATE_STATS_EVERY=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "formed-brazil",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##############################################################################| 5/5 [02:40<00:00, 32.07s/episodes]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ep_rewards=[]\n",
    "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
    "\n",
    "\n",
    "    # Restarting episode - reset episode reward and step number\n",
    "    episode_reward = 0\n",
    "    step = 1\n",
    "\n",
    "    # Reset environment and get initial state\n",
    "    current_state = env.reset()\n",
    "\n",
    "    # Reset flag and start iterating until episode ends\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "        # This part stays mostly the same, the change is to query a model for Q values\n",
    "        if np.random.random() > epsilon:\n",
    "            # Get action from Q table\n",
    "            action = np.argmax(agent.get_qs(current_state,step))\n",
    "        else:\n",
    "            # Get random action\n",
    "            action = np.random.randint(0, env.ACTION_SPACE_SIZE)\n",
    "\n",
    "        new_state, reward, done = env.step(action)\n",
    "        \n",
    "\n",
    "        # Transform new continous state to new discrete state and count reward\n",
    "        episode_reward += reward\n",
    "\n",
    "        #if SHOW_PREVIEW and not episode % AGGREGATE_STATS_EVERY:\n",
    "            #env.render()\n",
    "\n",
    "        # Every step we update replay memory and train main network\n",
    "        agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "        agent.train(done, step)\n",
    "\n",
    "        current_state = new_state\n",
    "        step += 1\n",
    "\n",
    "    # Append episode reward to a list and log stats (every given number of episodes)\n",
    "    ep_rewards.append(episode_reward)\n",
    "    if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
    "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        \n",
    "        # Save model, but only when min reward is greater or equal a set value\n",
    "        #if min_reward >= MIN_REWARD:\n",
    "            #agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= eps_decay\n",
    "        epsilon = max(MIN_EPSILON, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-marketing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(self.energy_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
